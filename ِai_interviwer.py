# -*- coding: utf-8 -*-
"""ِAI_Interviwer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a3PovZ2nInwpsLohVLb5sdkXFNaGp83e

#**Ollama**
"""

!nvidia-smi

!apt-get update
!apt-get install -y curl lspci

!curl -fsSL https://ollama.com/install.sh | sh

!ollama run llama3:8b

import subprocess
subprocess.Popen("ollama serve",shell=True)

!pip install langchain-community pypdf

# load data
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import TextLoader

pdf = PyPDFLoader('/content/ShroukAdel_CV.pdf')
cv =pdf.load()

loader = TextLoader('/content/Job_descriptionn.txt')
Job_des = loader.load()


print(cv[0].page_content)

from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
import subprocess
import time

# === Step 1: Start the Ollama server ===
subprocess.Popen("ollama serve", shell=True)
#time.sleep(15)  # Wait for Ollama to load the model

# === Step 2: Initialize the LLM ===
llm = Ollama(model="llama3:8b")

# === Step 3: Load CV and Job Description ===
cv_text = "\n".join([doc.page_content for doc in cv])
job_desc_text = "\n".join([doc.page_content for doc in Job_des])

memory = ConversationBufferMemory(memory_key="history", return_messages=True)

# === Step 5: Create the prompt template ===
question_prompt_template ='\n'.join(['You are an AI interviewer conducting a job interview.',
                            "Your task is to generate exactly 5 questions, one at a time, based on the candidate's resume and the job description",
                            "Each question should be relevant to the candidate's experience, skills, or the job requirements",
                            "Do not ask more than one question at a time. Only output the question text, nothing else. If the candidate has answered previous questions,",
                            " use the conversation history to avoid repeating topics and ensure questions are contextual.",
                            "immportant note: be focus more on your questions on Job descriptions ",
                            "important note:you MUST start the interview with wellcome message if this is  question number:{question_number} =1 like Hello"
                             "Don't include any 'think' or reasoning—just output the question only."
                            'Resume:{cv_text}',
                            'Job description :{job_desc_text}',
                            'conversation history:{history}',
                            "question number:{question_number}"])

evaluation_prompt_template='\n'.join(["You are an AI interviewer evaluating a candidate's interview performance",
                                      "Based on the candidate's resume, the job description,",
                                      " and their responses to 5 interview questions,",
                                      "assess how well their answers demonstrate suitability for the job",
                                      "Provide a score from 0 to 100 ,where 100 indicates a perfect match",
                                      "Also, provide a brief explanation (2-3 sentences) of the score.",
                                      "thank the candidate as end of the interview for his time before your evaluation."
                                      'Resume:{cv_text}',
                                      'Job description :{job_desc_text}',
                                      'conversation history:{history}',
                                      'Output the result in the following format',
                                      "Score: [number]",
                                      "Explanation: [text]"])

question_prompt = ChatPromptTemplate.from_template(question_prompt_template)
evaluation_prompt = ChatPromptTemplate.from_template(evaluation_prompt_template)

# build chain
question_chain = (
    question_prompt
    | llm
)
evaluation_chain =(
     evaluation_prompt
     | llm
)

def conduct_interview():
  question_num =1
  max_num_questions = 5

  while question_num <= max_num_questions:

    # Load current conversation history
        current_history = memory.load_memory_variables({})["history"]

        question = question_chain.invoke({
            "cv_text": cv_text,
            "job_desc_text": job_desc_text,
            "history": current_history,
            "question_number": question_num
        })

        print(f"Question {question_num}: {question}")

        user_reply =input('Your answer:')
        # save question and answer in the memory
        memory.save_context(
            {"question": question},
             {"answer": user_reply}
            )

        question_num +=1

  # evaluate response and compute score
  current_history = memory.load_memory_variables({})["history"]

  evaluation_result = evaluation_chain.invoke({
       "cv_text": cv_text,
       "job_desc_text": job_desc_text,
       "history": current_history
   })


  print(f"Evaluation Result: {evaluation_result}")

conduct_interview()

from google.colab import userdata
ngrok_key = userdata.get('ngrock')
port = 5000

pip install PyMuPDF

from flask import Flask, request, jsonify
import fitz  # PyMuPDF
import os
from pyngrok import ngrok

app = Flask(__name__)
ngrok.set_auth_token(ngrok_key)
run_with_ngrok(app)
app.config['UPLOAD_FOLDER'] = 'uploads'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# Dummy memory and chain logic
memory = {
    "history": [],
    "load_memory_variables": lambda _: {"history": memory["history"]},
    "save_context": lambda question, answer: memory["history"].append({**question, **answer})
}

class MockChain:
    def invoke(self, inputs):
        if "question_number" in inputs:
            return f"What is your experience related to question {inputs['question_number']}?"
        else:
            return {"score": 88, "feedback": "Impressive, relevant experience."}

question_chain = MockChain()
evaluation_chain = MockChain()

# Extract text from uploaded PDF
def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    return "\n".join([page.get_text() for page in doc])
ngrok.connect(port).public_url
@app.route('/interview', methods=['POST'])
def conduct_interview():
    if 'cv' not in request.files or 'job_desc' not in request.files:
        return jsonify({'error': 'Both CV and Job Description PDF files are required.'}), 400

    cv_file = request.files['cv']
    job_file = request.files['job_desc']
    answers = request.form.getlist('answers')

    if len(answers) != 5:
        return jsonify({'error': 'You must provide exactly 5 answers.'}), 400

    # Extract text from PDFs
    cv_text = extract_text_from_pdf(cv_file)
    job_desc_text = extract_text_from_pdf(job_file)

    # Interview process
    question_num = 1
    response_questions = []

    while question_num <= 5:
        current_history = memory["load_memory_variables"]({})["history"]

        question = question_chain.invoke({
            "cv_text": cv_text,
            "job_desc_text": job_desc_text,
            "history": current_history,
            "question_number": question_num
        })

        response_questions.append(question)

        memory["save_context"](
            {"question": question},
            {"answer": answers[question_num - 1]}
        )

        question_num += 1

    current_history = memory["load_memory_variables"]({})["history"]
    evaluation_result = evaluation_chain.invoke({
        "cv_text": cv_text,
        "job_desc_text": job_desc_text,
        "history": current_history
    })

    return jsonify({
        "questions": response_questions,
        "evaluation": evaluation_result
    })

if __name__ == '__main__':
    app.run(port=5000)

import json
import pandas as pd
from tensorflow.keras.models import load_model
from flask import Flask, request, jsonify
import numpy as np
import tensorflow as tf
from flask_ngrok import run_with_ngrok
from pyngrok import ngrok
from flask import Flask, request, render_template, jsonify

app = Flask(__name__)
ngrok.set_auth_token(ngrok_key)
run_with_ngrok(app)
#app = Flask(_name_)
#model = tf.keras.models.load_model('plants.h5')
# Define your function to make predictions
def respond(input_sentence):
    # Assuming you have a function called predict to make predictions
    pred =conduct_interview(input_sentence)
    return pred
ngrok.connect(port).public_url
# Define a route for your API
#@app.route("/check", methods=["POST"])
@app.route('/predictAPI', methods=['POST'])
def create_page():
    # Assuming you have input_sentence available in the request
    input_sentence = request.json.get('input_sentence', '')
    label = respond(input_sentence)
    print (label)
    return jsonify({'label': label})

if __name__ == "__main__":
    app.run()